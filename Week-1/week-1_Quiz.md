**1.** Interacting with Large Language Models (LLMs) differs from traditional machine learning models. Working with LLMs involves natural language input, known as a _____, resulting in output from the Large Language Model, known as the ______.

Choose the answer that correctly fill in the blanks.
- [ ] tunable request, completion
- [x] prompt, completion
- [ ] prediction request, prediction response
- [ ] prompt, fine-tuned LLM

**2.** Large Language Models (LLMs) are capable of performing multiple tasks supporting a variety of use cases. Which of the following tasks supports the use case of converting code comments into executable code?
- [x] Translation
- [ ] Information Retrieval
- [ ] Text summarization
- [ ] Invoke actions from text

**3.** What is the self-attention that powers the transformer architecture?
- [ ] The ability of the transformer to analyze its own performance and make adjustments accordingly.
- [x] A mechanism that allows a model to focus on different parts of the input sequence during computation.
- [ ] A measure of how well a model can understand and generate human-like language.
- [ ] A technique used to improve the generalization capabilities of a model by training it on diverse datasets.

**4.** Which of the following stages are part of the generative AI model lifecycle mentioned in the course? (Select all that apply)
- [x] Deploying the model into the infrastructure and integrating it with the application.
- [x] Defining the problem and identifying relevant datasets.
- [ ] Performing regularization
- [x] Manipulating the model to align with specific project needs.
- [x] Selecting a candidate model and potentially pre-training a custom model.

**5.** "RNNs are better than Transformers for generative AI Tasks."

Is this true or false?
- [ ] True
- [x] False

**6.** Which transformer-based model architecture has the objective of guessing a masked token based on the previous sequence of tokens by building bidirectional representations of the input sequence.
- [x] Autoencoder
- [ ] Autoregressive
- [ ] Sequence-to-sequence

**7.** Which transformer-based model architecture is well-suited to the task of text translation?
- [ ] Autoencoder
- [ ] Autoregressive
- [x] Sequence-to-sequence

**8.** Do we always need to increase the model size to improve its performance?
- [ ] True
- [x] False

**9.** Scaling laws for pre-training large language models consider several aspects to maximize performance of a model within a set of constraints and available scaling choices. Select all alternatives that should be considered for scaling when performing model pre-training?
- [ ]Batch size: Number of samples per iteration 
- [x] Model size: Number of parameters
- [x] Dataset size: Number of tokens
- [x] Compute budget: Compute constraints

**10.** "You can combine data parallelism with model parallelism to train LLMs."

Is this true or false?
- [x] True
- [ ] False
